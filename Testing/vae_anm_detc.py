import torch
import numpy as np
import torch.nn as nn
import os
from ccbdl.utils import DEVICE
from ccbdl.storages import storages
from Data.prepare_data import prepare_data
from sklearn.metrics import roc_auc_score
from sklearn.metrics import classification_report, average_precision_score
import matplotlib.pyplot as plt
from sklearn import metrics
from Plotting.anomaly_plot import testdata_plotting
from torch.distributions import Normal
from Plotting.latentspace_plotting import visualize_latent_space


class VAEAnomalyDetection():
        
    def __init__(self, path, study_config: dict, data_config: dict):    
                  
        self.path = path
        self.threshold = None
        self.pred_labels = None
        self.testt_dataa = None
        self.study_config = study_config
        self.data_config = data_config
                                                      
        # get data
        train_data, test_data, val_data = prepare_data(self.data_config)

        self.train_data = train_data                     
        self.test_data = test_data
        self.val_data  = val_data
        
        # For storage of results and plots    
        self.parameter_storage = storages.ParameterStorage(path)
        self.parameter_storage.write("This file is automatically generated by ccbdl.learning.base.BaseLearning")
        self.figure_storage = storages.FigureStorage(path, dpi=300, types=("png", "pdf"))
        
        # Path to retrive the best model
        self.path_best = os.path.join(self.path,"best_state.pt")
        self.path_bestmodel =  os.path.join(self.path,"best_model.pt")

#----------------------------------------------------------------------------------------------                        
    def find_threshold(self):
        
        val_labels = []
        loss_crit = nn.MSELoss(reduction = 'none')
        
        print("\n******** Finding threshold reconstruction error of the Data ********\n")                                

        # Loading the saved best model along with hidden size and weights
        self.model = torch.load(self.path_bestmodel)
        self.model.eval() 
        
        self.L = 10               
        val_data = []
        val_labels = []
        valdata_rec = []
        val_rec_mu = []
        val_rec_sigma = []
        val_probabilities = []
        latent_spacee = []
        
        sample_probabilitiess = [] 
        avg_tot_probabilitiess = [] 
        # Assuming self.test_data is your test data
        with torch.no_grad():
            for _, (inp, labels) in enumerate(self.test_data):
                                
                inp = inp.to(torch.float32)
                inp = inp.to(DEVICE)
                test_reconstructions = self.model(inp)
                
                for i in range(inp.size(0)):
                    sample = inp[i, :, :]
                    sample = sample.view(sample.size(0), sample.size(0), sample.size(1))
                            
                    sample = sample.to(torch.float32)
                    sample = sample.to(DEVICE)
                    # network
                    enc = self.model.encoder(sample)
                    latent_mu = self.model.en_mu(enc)
                    latent_logvar = self.model.en_logvar(enc)
                    latent_logvar = torch.exp(0.5 * latent_logvar)
            
                    latent_dist = Normal(latent_mu, latent_logvar)
                    z = latent_dist.rsample([self.L])  # shape:[self.L,batch_size,1,latent_size]
                    z = z.view(-1, z.size(2), z.size(3))  # shape:[self.L*batch_size,1,latent_size]
                    
                    decoded = self.model.decoder(z)
                    recon_mu = self.model.de_mu(decoded)
                    recon_logvar = self.model.de_logvar(decoded)
                    recon_logvar = torch.exp(0.5 * recon_logvar)
                    # rec_z = self.model.reparameterize(recon_mu, recon_logvar)
                    recon_dist = Normal(recon_mu, recon_logvar)
                    p = recon_dist.log_prob(sample).exp().mean(dim=0) 
                    # p = recon_dist.log_prob(sample).exp().mean() 
                    sample_probabilitiess.append(p)
                
                val_data.append(inp) 
                val_labels.append(labels)
                valdata_rec.append(test_reconstructions) 
            
            # self.valdata_rec = torch.cat(valdata_rec, dim = 0).to(DEVICE)    
            # v_concatenated_prob = torch.cat(sample_probabilitiess, dim=0).to(DEVICE)                                     

    #         #--------------------------------------------------------------------------- 
    #         # Plot latent space dimensions
    #         latent_space = torch.cat(latent_space, dim=0)
    #         latent_space = latent_space.reshape(latent_space.shape[0],-1)
            
    #         # Plot latent space dimensions
    #         num_plots = latent_space.shape[-1]
    #         for i in range(num_plots-1):
    #             plt.figure(figsize=(10,10))
    #             plt.scatter(latent_space[:,i], latent_space[:,i+1], cmap="viridis")
    #             plt.xlabel(f"dim {i+1}")
    #             plt.ylabel(f"dim {i+2}")
    #             plt.title(f'(Val data Dimensions {i+1} and {i+2})')
    #             plt.colorbar()      
    #             plt.show() 
    #         #--------------------------------------------------------------------------- 
            
            # v_concatenated_rec_mu = torch.cat(total_rec_mu, dim=0).to(DEVICE)
            # v_concatenated_rec_logvar = torch.cat(total_rec_sigma, dim=0).to(DEVICE)
            v_concatenated_prob = torch.cat(sample_probabilitiess, dim=0).to(DEVICE) 
           
    #         v_recmu_array = v_concatenated_rec_mu.cpu().numpy() 
    #         v_recsig_array = v_concatenated_rec_logvar.cpu().numpy() 
            v_prob_array = v_concatenated_prob.cpu().numpy() 
            # avg_prob_density = v_concatenated_prob.mean(dim=0).mean(dim=-1)
            self.val_labels = torch.cat(val_labels)
            
            # Finding the best possible Threshold Value for Anomaly Detection              
            best_f1_score = 0.0
            best_threshold = 0.0
            
            # Iterate over different values of y
            start = 0.0001
            end = 0.5
            step = 0.0001
            
            current_value = start
            while current_value <= end:
                # print(current_value)
                threshold = float(current_value)
                v_pred_labels_tensor = (v_concatenated_prob < threshold).to(DEVICE)
                v_ground_truth_tensor_1d = self.val_labels.view(-1)
                v_preds_tensor_1d = v_pred_labels_tensor.view(-1)
                v_ground_truth = v_ground_truth_tensor_1d.cpu().numpy()
                v_final_preds = v_preds_tensor_1d.cpu().numpy()
                val_f1_score = metrics.f1_score(v_ground_truth, v_final_preds)
                
                current_value += step
                
                if val_f1_score > best_f1_score:
                    best_f1_score = val_f1_score
                    best_threshold = threshold

            self.threshold = best_threshold
            print("Best F1 Score from val dataset:", best_f1_score, '\n')
            print("Best Threshold from Val Data:", best_threshold, '\n')
            
        return best_threshold
    
#------------------------------------------------------------------------------------------------- 

#Testing Phase
 
    def find_anomalies(self):
        
        self.model = torch.load(self.path_bestmodel)
        self.model.eval() 
        loss_crit_test = nn.MSELoss(reduction = 'none')  
        # loss_crit_test = nn.BCELoss(reduction = 'none')  
        
        self.L = 10               
        test_data = []
        test_labels = []
        testdata_rec = []
        test_rec_mu = []
        test_rec_sigma = []
        test_probabilities = []
        latent_spacee = []
        # with torch.no_grad():
        #     for _, (inp,labels) in enumerate(self.test_data):                            
        #         # get data
        #         inp = inp.to(torch.float32)
        #         inp = inp.to(DEVICE)                 
        #         # network
                
        #         test_reconstructions = self.model(inp) 
                
        #         #------------------------------------------------------------------------
        #         enc = self.model.encoder(inp)
        #         latent_mu = self.model.en_mu(enc)
        #         latent_logvar = self.model.en_logvar(enc)
        #         latent_logvar  = torch.exp(0.5 * latent_logvar)
                
        #         latent_dist = Normal(latent_mu, latent_logvar)
        #         z = latent_dist.rsample([self.L])  # shape: [L, batch_size, latent_size]
        #         z  = (z.size(0) * z.size(1),) + z.size()[2:]
                
        #         for i in enumerate(z): 
                
        #             # z = self.model.reparameterize(latent_mu, latent_logvar)
        #             # latent_spacee.append(z)
        #             decoded = self.model.decoder(i)
        #             recon_mu =  self.model.de_mu(decoded)
        #             recon_logvar = self.model.de_logvar(decoded)
        #             recon_logvar  = torch.exp(0.5 * recon_logvar)
        #             test_rec_mu.append(recon_mu)
        #             test_rec_sigma.append(recon_logvar)
        #             recon_dist = Normal(recon_mu, recon_logvar )
        #             p = recon_dist.log_prob(inp).exp.mean(dim=0).mean(dim=-1)  # vector of shape [batch_size]
        #             test_probabilities.append(p)
                
        #         # rec_z = self.model.reparameterize(recon_mu, recon_logvar)
        #         # mse_loss = loss_crit_test(test_reconstructions, inp)
        #         # test_rec_mu.append(recon_mu)
        #         # test_rec_sigma.append(recon_logvar)
        #         # recon_dist = Normal(recon_mu, recon_logvar )
        #         # p = recon_dist.log_prob(inp).exp()  # vector of shape [batch_size]
        #         # test_probabilities.append(p)
        #         # test_data.append(inp) 
        #         # test_labels.append(labels) 
        #         # testdata_rec.append(test_reconstructions)                                      

        
        sample_probabilities = [] 
        avg_tot_probabilities = [] 
        # Assuming self.test_data is your test data
        with torch.no_grad():
            for _, (inp, labels) in enumerate(self.test_data):
                                
                inp = inp.to(torch.float32)
                inp = inp.to(DEVICE)
                test_reconstructions = self.model(inp)
                
                for i in range(inp.size(0)):
                    sample = inp[i, :, :]
                    sample = sample.view(sample.size(0), sample.size(0), sample.size(1))
                            
                    sample = sample.to(torch.float32)
                    sample = sample.to(DEVICE)
                    # network
                    enc = self.model.encoder(sample)
                    latent_mu = self.model.en_mu(enc)
                    latent_logvar = self.model.en_logvar(enc)
                    latent_logvar = torch.exp(0.5 * latent_logvar)
            
                    latent_dist = Normal(latent_mu, latent_logvar)
                    z = latent_dist.rsample([self.L])  # shape:[self.L,batch_size,1,latent_size]
                    z = z.view(-1, z.size(2), z.size(3))  # shape:[self.L*batch_size,1,latent_size]
                    
                    decoded = self.model.decoder(z)
                    recon_mu = self.model.de_mu(decoded)
                    recon_logvar = self.model.de_logvar(decoded)
                    recon_logvar = torch.exp(0.5 * recon_logvar)
                    # rec_z = self.model.reparameterize(recon_mu, recon_logvar)
                    recon_dist = Normal(recon_mu, recon_logvar)
                    p = recon_dist.log_prob(sample).exp().mean(dim=0) 
                    # p_avg = recon_dist.log_prob(sample).exp().mean(dim=0).mean(dim=-1) 
                    # p = recon_dist.log_prob(sample).exp().mean() 
                    sample_probabilities.append(p)
                    # avg_tot_probabilities.append(p_avg)
                
                test_data.append(inp) 
                test_labels.append(labels)
                testdata_rec.append(test_reconstructions) 
            
            self.testdata_rec = torch.cat(testdata_rec, dim = 0).to(DEVICE)    
            t_concatenated_prob = torch.cat(sample_probabilities, dim=0).to(DEVICE)   
                    
                    # for i in range(L):
                    #     latent_dist = Normal(latent_mu, latent_logvar)
                    #     z_sample = latent_dist.rsample()  # shape: [batch_size, latent_size]
            
                    #     decoded = self.model.decoder(z_sample)
                    #     recon_mu = self.model.de_mu(decoded)
                    #     recon_logvar = self.model.de_logvar(decoded)
                    #     recon_logvar = torch.exp(0.5 * recon_logvar)
            
                    #     recon_dist = Normal(recon_mu, recon_logvar)
                    #     p = recon_dist.log_prob(inp).exp().mean(dim=0).mean(dim=-1)  # vector of shape [batch_size]
                    #     sample_probabilities.append(p)


    #--------------------------------------------------------------------------- 
            # # Plot latent space dimensions
            # latent_spacee = torch.cat(latent_spacee, dim=0)
            # latent_spacee = latent_spacee.reshape(latent_spacee.shape[0],-1)
            
            # # Plot latent space dimensions
            # num_plots = latent_spacee.shape[-1]
            # for i in range(num_plots-1):
            #     plt.figure(figsize=(10,10))
            #     plt.scatter(latent_spacee[:,i], latent_spacee[:,i+1], cmap="viridis")
            #     plt.xlabel(f"dim {i+1}")
            #     plt.ylabel(f"dim {i+2}")
            #     plt.title(f'(Test data Dimensions {i+1} and {i+2})')
            #     plt.colorbar()      
            #     plt.show() 
            #--------------------------------------------------------------------------- 

            # self.testdata_rec = torch.cat(testdata_rec, dim = 0).to(DEVICE)
            # t_concatenated_rec_mu = torch.cat(test_rec_mu, dim=0).to(DEVICE)
            # t_concatenated_rec_logvar = torch.cat(test_rec_sigma, dim=0).to(DEVICE)
            # t_concatenated_prob = torch.cat(test_probabilities, dim=0).to(DEVICE) 
           
            # v_recmu_array = t_concatenated_rec_mu.cpu().numpy() 
            # v_recsig_array = t_concatenated_rec_logvar.cpu().numpy() 
            v_prob_array = t_concatenated_prob.cpu().numpy() 
            # avg_prob_density = t_concatenated_prob.mean(dim=0).mean(dim=-1)
                                
           # testdata_rec_array = self.testdata_rec.cpu() 
            self.test_data_tensor = torch.cat(test_data, dim = 0)
            self.test_labels = torch.cat(test_labels)                     
                       
            threshold = self.threshold
            # threshold = 0.0001                              
            anomaly = (t_concatenated_prob < threshold).to(DEVICE)    
            # anomaly = (t_concatenated_prob > threshold).to(DEVICE)
            # 1 = anomaly, 0 = normal            
            pred_labels_tensor = torch.where(anomaly, torch.tensor(1).to(DEVICE), torch.tensor(0).to(DEVICE))                         
            self.pred_labels = pred_labels_tensor
                       
        # ---------------------------------------------------------------------------------        
        #Evaluating the test data based on some metrics
        
            actual_labels_tensor = self.test_labels
            
            # Reshape the original labels tensor into a 1D array
            ground_truth_tensor_1d = actual_labels_tensor.view(-1)
            
            # Count the number of zeros and ones
            normal_count = torch.sum(ground_truth_tensor_1d == 0)
            print("no of actual normal data points are:", normal_count, '\n')
            anomaly_count = torch.sum(ground_truth_tensor_1d == 1)                                
            print("no of actual anomaly data points are:", anomaly_count,'\n' ) 
            
            ground_truth = ground_truth_tensor_1d.cpu().numpy()
                        
            # Reshape the predictions label tensor into a 1D array
            preds_tensor_1d = pred_labels_tensor.view(-1)
            
            normal_count = torch.sum(preds_tensor_1d == 0)
            print("no of predicited normal data points are:", normal_count, '\n') 
            anomaly_count = torch.sum(preds_tensor_1d == 1)                                           
            print("no of predicted anomaly data points are:", anomaly_count,'\n' ) 
            
            final_preds = preds_tensor_1d.cpu().numpy()
            
            roc_auc = roc_auc_score(ground_truth, final_preds)         
            print("AUROC Score is:", roc_auc)
            roc_str = "AUROC Score is: " + str(roc_auc)
                       
            fpr, tpr, thresholds = metrics.roc_curve(ground_truth, final_preds)          
            fig = metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc, estimator_name='roc_auc_curve')
            fig.plot()
            plt.show()                   
            report = classification_report(ground_truth, final_preds, digits=4)
            print(report,'\n')

            #Calculatimg AUPRC
            auprc = average_precision_score(ground_truth, final_preds)
            print('AUPRC Score is', auprc)
            prc_str = "AUPRC Score is: " + str(auprc)        
            
            self.parameter_storage.write_tab("Classification Report", str(report))
            self.parameter_storage.write_tab("00", str(roc_str))
            self.parameter_storage.write_tab("00", str(prc_str))
            
            # Plotting the data and visualizing anomalies
            testdata_plotting(path = self.path,
                              test_data_tensor = self.test_data_tensor,   
                              testdata_rec = self.testdata_rec, 
                              test_labels = self.test_labels, 
                              pred_labels =self.pred_labels, 
                              figure_storage = self.figure_storage)
                        
        return actual_labels_tensor, pred_labels_tensor
